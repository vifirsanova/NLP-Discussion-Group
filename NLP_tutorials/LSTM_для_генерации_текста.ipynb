{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM для генерации текста.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teXnRHe1a9tL",
        "colab_type": "text"
      },
      "source": [
        "*Материал оформила и подготовила* Виктория Фирсанова\n",
        "\n",
        "# **Обучение LSTM для генерации текста**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAESfWDhcig0",
        "colab_type": "text"
      },
      "source": [
        "# Импорт классов и функций"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3ainuQobaKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy # NumPy-массивы будут использоваться для работы с обучающей выборкой\n",
        "\n",
        "# импортируем из библиотеки Keras функции для построения нейросети\n",
        "\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM \n",
        "from tensorflow.keras.callbacks import ModelCheckpoint #эта функция позволит сохранить вычисленные сетью веса\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IJ0QZhadX9h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Загрузим данные для обучения\n",
        "\n",
        "Все загруженные или полученные в результате работы программы файлы в *Google.Colab* можно увидеть (а также скачать или удалить) во вкладе *Files* слева.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpPQdvWhcvSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # функция для загрузки любых файлов\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J7Ec7xMe0BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls # проверка: отображает список имен загруженных файлов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfgZCrJUedik",
        "colab_type": "text"
      },
      "source": [
        "# Загрузим текст и посмотрим на наши обучающие данные\n",
        "\n",
        "Предварительно наши данные придется немного обработать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETYZA_UAcenh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = \"output.txt\" # имя файла с обучающей выборкой\n",
        "raw_text = open(filename).read()\n",
        "\n",
        "# Все символы можно привести к нижнему регистру.\n",
        "# Это уменьшит словарный запас, который наша сеть должна будет выучить.\n",
        "\n",
        "raw_text = raw_text.lower() \n",
        "\n",
        "# В ходе экспериментов я выяснила, что моя выборка слишком объемная для этой задачи, \n",
        "# поэтому мне понадобилось ее \"обрезать\".\n",
        "# Мне также пришлось избавиться от всех символов кроме букв и цифр.\n",
        "# Их наличие влияет на длину словаря - сеть будет обучаться с большим трудом, если словарь окажется слишком длинным.\n",
        "# В моем случае с интернет-текстами - это особенно важно. \n",
        "# Кроме обычных знаков препинания, в выборку могут попасть и другие символы, например, эмодзи."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdZrJ0SJhx7C",
        "colab_type": "text"
      },
      "source": [
        "# Подготовка данных для обучения\n",
        "\n",
        "Мы не можем построить модель из символов (*букв и знаков препинания*) напрямую. Но мы можем преобразовать символы в числа.\n",
        "\n",
        "Для обучения нейросети мы будем использовать посимвольную модель."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_1uN5S2fKvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Преобразуем уникальные символы в целые числа\n",
        "\n",
        "chars = sorted(list(set(text))) # используем метод set()\n",
        "\n",
        "# метод конвертирует любой итерируемый объект (iterable) в отдельный элемент\n",
        "# получаем список символов, сортированный в алфавитном порядке\n",
        "\n",
        "print(\"Взглянем на получившийся список символов:\", chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBKOnN8re645",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Получим словарь символов и присвоим каждому элементу этого словаря индекс.\n",
        "# На предыдущем этапе мы отсортировали элементы словаря в алфавитном порядке.\n",
        "# Теперь порядковый номер каждого элемента в этом списке \"кодирует\" соответствующий символ.\n",
        "\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars)) \n",
        "\n",
        "n_chars = len(text)\n",
        "n_vocab = len(chars)\n",
        "\n",
        "print(\"Количество символов в базе данных для обучения: \", n_chars)\n",
        "print(\"Длина словаря символов: \", n_vocab)\n",
        "print(\"Словарь:\", char_to_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s3l6O65HNPy",
        "colab_type": "text"
      },
      "source": [
        "# Подготовим выборку \n",
        "- Разделим базу данных на последовательности текстов заданной длины. \n",
        "- Преобразуем символы в целые числа, используя созданный на предыдущем этапе частотный словарь."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_ua8D2tjAe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100 # задаем длину отрывков для обучения\n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range(0, n_chars - seq_length, 1): \n",
        "\n",
        "\tseq_in = text[i:i + seq_length]\n",
        "\tseq_out = text[i + seq_length]\n",
        "\n",
        "\tdataX.append([char_to_int[char] for char in seq_in]) \n",
        "\tdataY.append(char_to_int[seq_out])\n",
        " \n",
        "n_patterns = len(dataX) \n",
        "\n",
        "print(\"Всего шаблонов для обучения: \", n_patterns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjFytVe9IpG9",
        "colab_type": "text"
      },
      "source": [
        "# Тренировочные данные подготовлены!\n",
        "Теперь нужно преобразовать их так, чтобы они подходили для работы с Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHYLOdzajLnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Преобразуем список входных последовательностей к виду [samples (образцы), time steps (временные шаги), features(особенности)].\n",
        "#Это вид, подходящий для обучения сети LSTM.\n",
        "\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "#Нормализация\n",
        "\n",
        "#Нам нужно изменить масштаб целых чисел так, чтобы LSTM могла \"исследовать\" данные.\n",
        "#Поскольку по умолчанию мы используем функцию активации сигмоид: 0 <= n <= 1\n",
        "\n",
        "#В ходе преобразований нам нужно получить \"одну горячую кодировку\" (one hot encoding).\n",
        "#Это более простое представление: разреженный вектор длиной n_vocab, полный нулей.\n",
        "#Вектор содержит одну единицу, которая представляет собой искомый символ.\n",
        "\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "#Приводим наши паттерны к виду \"one hot encoding\" (пример: если n=3, то выглядеть это будет так: [0. 0. 1. 0.])\n",
        "#Cейчас они представлены в виде чисел, каждое из которых представляет собой индекс частотности данного символа.\n",
        "\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SgCAOcQUue2",
        "colab_type": "text"
      },
      "source": [
        "# Обучаем LSTM-модель\n",
        "\n",
        "Параметры модели можно (и нужно, если мы хотим получить лучший результат) изменять. \n",
        "\n",
        "Мы сохраняем веса модели после каждой эпохи обучения, можно смело переобучать нейросеть и экспериментировать с параметрами! \n",
        "\n",
        "*Все наши веса сохраняться во вкладке \"Files\": лучшие следует сохранить, а худшие - удалить.*\n",
        "\n",
        "**Как долго обучается модель?**\n",
        "\n",
        "Все завивисит от количества эпох, длины словаря и объема обучающей выборки. Эту модель можно назвать неторопливой. Она может обучаться и 2 часа, и 12. Но через 12 часов карета превратится в тыкву, и сессия в Google.Colab прервется.\n",
        "\n",
        "**Как определить лучший вариант?**\n",
        "\n",
        "Из всех вариантов, полученных в ходе обучения, лучшим будет тот, в котором меньше всего потерь => мы ищем минимальное значение параметра *loss*.\n",
        "\n",
        "В следующем примере *loss* первой эпохи равен 3.1670, а второй - 3.0254. Соответственно, нам нужны веса, который сохарнились в файл с названием *weights-improvement-02-3.0254.hdf5*.\n",
        "\n",
        "```\n",
        "Epoch 1/2\n",
        "2343/2343 [==============================] - ETA: 0s - loss: 3.1670\n",
        "Epoch 00001: loss improved from inf to 3.16704, saving model to weights-improvement-01-3.1670.hdf5\n",
        "2343/2343 [==============================] - 1454s 621ms/step - loss: 3.1670\n",
        "\n",
        "Epoch 2/2\n",
        "2343/2343 [==============================] - ETA: 0s - loss: 3.0254\n",
        "Epoch 00002: loss improved from 3.16704 to 3.02536, saving model to weights-improvement-02-3.0254.hdf5\n",
        "2343/2343 [==============================] - 1456s 621ms/step - loss: 3.0254\n",
        "```\n",
        "\n",
        "**За счет каких параметров я могу улучшить качество работы модели?**\n",
        "\n",
        "Вот несколько идей, которые сработали с моими данными:\n",
        "- увеличить количество эпох (10 -> 20 -> 50 -> 100);\n",
        "- уменьшить batch size (128 -> 64);\n",
        "- добавить еще один слой: \n",
        "```\n",
        "model.add(LSTM(256)) \n",
        "```\n",
        "\n",
        "**Как увеличить скорость обучения?**\n",
        "\n",
        "Вот что мне помогло:\n",
        "\n",
        "- я удалила символы из обучающей выборки\n",
        "- и уменьшила ее объем\n",
        "\n",
        "**НО!** из-за малого объема выборки, НС будет скорее воспроизводить тексты из обучающей выборки, чем генерировать оригинальные.\n",
        "\n",
        "*Цель:* составить не очень большую, но емкую выборку, на основе которой программа могла бы генерировать оригинальные тексты.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk7jOtnhjflA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Задаем модель\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Задаем параметры для сохранения весов модели по ходу обучения\n",
        "\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faz7fE9oU36h",
        "colab_type": "text"
      },
      "source": [
        "# Генерация текста с помощью LSTM-модели\n",
        "\n",
        "- Загружаем веса, которые вычислила нейросеть в ходе обучения\n",
        "- Компилируем модель\n",
        "- Пишем функцию для генерации текста\n",
        "- Наблюдаем за результатом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHeOX7pRKEOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.upload() # при необходимости загружаем предобученную модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_xTC5WanAAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# загружаем файл весов с наименьшим значением потерь\n",
        "\n",
        "filename = \"weights-improvement-40-1.2724.hdf5\"\n",
        "model.load_weights(filename) \n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam') # и компилируем модель"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB9gI-rdn7om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Чтобы сопоставить символы с целыми числами,\n",
        "#создаем \"обратное отображение\" (char to integer -> integer to char).\n",
        "\n",
        "#Это нужно для того, чтобы мы, homo sapiens, могли понять предсказания искусственного интеллекта :)\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6BaAiW0n9aO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Генерация текста = прогнозы нашей модели\n",
        "\n",
        "# Задается случайная последовательность чисел.\n",
        "# Она используется для генерации следующего символ.\n",
        "# Последовательность чисел обновляется, сгенерированный символ добавляется в конец, первый - удаляется. \n",
        "\n",
        "# Процесс повторяется до выполнения заданного условия.\n",
        "# Например, мы можем задать условие, при котором последовательность не должна превышать 1000 символов.\n",
        "\n",
        "# Мы можем выбрать случайный шаблон в качестве источника для генерации последовательности символов.\n",
        "\n",
        "import sys\n",
        "\n",
        "# Выбираем из обучающей выборки случайный источник для генерации текста с помощью генератора случайных чисел.\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "# Генерируем текст!\n",
        "\n",
        "for i in range(5000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nEnd.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1zjASF0Nx6w",
        "colab_type": "text"
      },
      "source": [
        "**Список источников:**\n",
        "\n",
        "1. Hochreiter, S. (1997). \"Long Short-term Memory\". Neural Computation 9(8): 1735-80. DOI: 10.1162/neco.1997.9.8.1735\n",
        "2. Brownlee, J. (2016). Text Generation With LSTM Recurrent Neural Networks in Python with Keras. Machine Learning Mastery. URL: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/ \n"
      ]
    }
  ]
}
